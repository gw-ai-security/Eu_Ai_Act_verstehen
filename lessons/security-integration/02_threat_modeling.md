---
title: Lesson 2 – Threat Modeling für KI-Systeme
version: 1.0
last_updated: 2025-08-21
tags: [Threat Modeling, STRIDE, LINDDUN, AI Act, Risk Assessment]
---

# Lernziele
- Methoden STRIDE & LINDDUN anwenden auf KI.
- AI-spezifische Bedrohungen (Prompt Injection, Data Poisoning).
- Relevante Artikel: Art. 9 (Risiken), Art. 15 (Security).

# Kerninhalte
- **STRIDE:** Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege.  
- **LINDDUN:** Privacy-Fokus: Linkability, Identifiability, Non-Repudiation, Detectability, Disclosure, Unawareness, Non-Compliance.  
- **AI Threats:** Model Theft, Data Poisoning, Evasion Attacks.

# Bloom-Aufgaben
- **Erinnern:** Nenne 3 AI-spezifische Bedrohungen.  
- **Verstehen:** Unterschied STRIDE vs. LINDDUN?  
- **Anwenden:** Erstelle ein Dataflow Diagramm für Predictive Maintenance.  
- **Analysieren:** Mappe Bedrohungen zu AI Act-Artikeln.  
- **Bewerten:** Welche Methode ist für Privacy-Risiken besser geeignet?  
- **Erschaffen:** Erstelle ein vollständiges Threat Model (DFD + STRIDE/LINDDUN).

# Praxisbeispiel
Predictive Maintenance System mit IoT-Sensoren.  
Threat: Datenvergiftung im Training.  
Control: Data Provenance & Versioning.  
