---
title: Lesson 3 – Red Teaming für KI-Systeme
version: 1.0
last_updated: 2025-08-21
tags: [Red Teaming, AI Security, AI Act, Adversarial Testing]
---

# Lernziele
- Rolle von Red Teaming bei AI-Security verstehen.
- Durchführung von Adversarial Tests.
- Relevante Artikel: Art. 15 (Robustness), Art. 21 (Monitoring).

# Kerninhalte
- **Red Team Ziele:** Schwachstellen aufdecken, Evasion & Poisoning simulieren.  
- **Methoden:** Prompt Injection Tests, Data Poisoning, Evasion Attacks, Model Inversion.  
- **AI Act Bezug:** Nachweis der Robustheit (Art. 15), kontinuierliches Monitoring (Art. 21).

# Bloom-Aufgaben
- **Erinnern:** Was ist ein Red Team?  
- **Verstehen:** Unterschied Red Teaming vs. Penetrationstests.  
- **Anwenden:** Simuliere eine Prompt Injection gegen ein HR-Chatbot.  
- **Analysieren:** Welche Evidenzen liefern Red Team Reports für Audits?  
- **Bewerten:** Grenzen von Red Teaming im AI-Kontext.  
- **Erschaffen:** Baue ein Adversarial Testing Playbook.

# Praxisbeispiel
Banking Fraud Detection System.  
Red Team simuliert Evasion mit adversarial samples → Modell täuscht Transaktionen als legitim.  
